{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/trump_sample_tweet.png\">\n",
    "Donald Trump’s many tweets during the Covid pandemic spread like wildfire, probably making him one of the most\n",
    "influential figures during the pandemic – but in the end, was he that influential? Will causal analyses of the effect of\n",
    "Trump’s tweets on Wikipedia and Google Trends pageviews show that he was leading or following online trends? Our\n",
    "goal is to study the impact an influential leader can have on information spread in a\n",
    "crisis with a focus on fake news, as an overload of misleading or contradictory\n",
    "statements (an infodemic, as [WHO](https://www.who.int/health-topics/infodemic#tab=tab_1) calls it) are known to have a detrimental impact on crisis\n",
    "management. To provide a more comprehensive insight into Trump’s actual influence\n",
    "on online information spread, we would then like to compare it with that of other\n",
    "factors such as mobility restrictions or key milestones (e.g. first Covid death). Trump\n",
    "the Trend Maker or Trump the Follower, that is the question!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Etienne/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import wordcloud \n",
    "from wordcloud import WordCloud\n",
    "from causalimpact import CausalImpact\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from pprint import pprint\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Local Modules\n",
    "from extra_material.wiki_pageviews_covid.analyses.helpers.load import load_interventions, load_aggregated, load_topics, load_pca\n",
    "from extra_material.wiki_pageviews_covid.analyses.helpers.vars import codes, helper_langs, interventions_helper, int_c, int_ls\n",
    "from extra_material.wiki_pageviews_covid.analyses.helpers.plot import set_size, plot_dates, plot_intervention, plot_cumm_diff\n",
    "from extra_material.wiki_pageviews_covid.analyses.helpers.pca import get_attention_shift_ts \n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from extra_material.wiki_gtrend_visualization import load_wikipedia_df, load_gtrend_df\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Granger test between GTrends views and tweets\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "#vader\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 0: Pre-processing\n",
    "\n",
    "### A) We start by preparing and familiarizing with our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our datasets :\n",
    "- Covid dataset from ada class\n",
    "- Donald Trump's tweet between 2019 and\n",
    "  2021: [Kaggle](https://www.kaggle.com/datasets/codebreaker619/donald-trump-tweets-dataset)\n",
    "- List of known fake news and their Google trends'\n",
    "  reference : [GitHub](https://github.com/epfl-dlab/fact-checkers-fact-check/blob/main/data/kg_ids.json)\n",
    "- Data extracted from Google Trends and Wikipedia (will be done after)\n",
    "\n",
    "Let's present Donald Trump's tweets dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a personalyze data parser\n",
    "custom_date = lambda x: datetime.strptime(x.split()[0], '%Y-%m-%d')\n",
    "df = pd.read_csv('extra_material/tweets.csv', parse_dates=['date'], date_parser=custom_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains all his tweets and retweets from 2009 to January, 8 2021 (one day before he was banned from Twitter) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no empty row in the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows with Nan value is', len(df)-(~df.isna()).prod(axis= 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the Covid period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df[df['date'] >= '2019-12-01']\n",
    "df_covid.sort_values(by='date')\n",
    "df_covid.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether Donald Trump Tweeted every day :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_span = pd.date_range(start = min(df_covid['date']), end = max(df_covid['date']), freq='D')\n",
    "\n",
    "data=df_covid.copy()\n",
    "data = pd.merge(data, pd.DataFrame(time_span, columns=['date']), on='date', how='right')\n",
    "\n",
    "print('Number of days where he did not tweet ' , data['id'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one day where he didn't tweet, we can drop this day from our dataset, it won't affect our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "data['per_day_tweets']= data.groupby('date')['date'].transform('count')\n",
    "data['per_day_retweets']= data.groupby('date')['retweets'].transform('sum')\n",
    "data['per_day_likes']= data.groupby('date')['favorites'].transform('sum')\n",
    "data_t = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the temporal evolutions of Trump's tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 3, ncols = 1, sharex=True, sharey=False)\n",
    "\n",
    "axes[0].plot(data['date'], data['per_day_tweets'])\n",
    "axes[0].set_title('Tweets per day', size=15)\n",
    "axes[1].plot(data['date'], data['per_day_retweets']/10**6)\n",
    "axes[1].set_title('Retweets per day (in millions)', size=15)\n",
    "axes[2].plot(data['date'], data['per_day_likes']/10**6)\n",
    "axes[2].set_title('Likes per day (in millions)', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have an idea of the type of tweets we have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=False, figsize=(8, 12))\n",
    "\n",
    "variables = ['isRetweet', 'isDeleted', 'isFlagged']\n",
    "colors = ['#FFCCCC', 'lightgreen']\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    ax = axes[i]\n",
    "    ax.pie(data[var].value_counts(), labels=['False', 'True'], autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "    ax.set_title(f'Distribution of {var}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very few deleted tweets, though this can be an indication of misinformation spreaded by Trump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spacy to get rid of stop words in the tweets. Stop words are all that words such as and, or, that, etc. that are not relevant for a tweet but are used very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before running following command install dictionary with:\n",
    "#python -m spacy download en_core_web_sm\n",
    "\n",
    "#download a english dictionary to make anlysis\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the most used words in Trump's tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = data.copy()\n",
    "\n",
    "df_clean['text'] = df_clean['text'].apply(lambda x: re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '',x))\n",
    "\n",
    "df_clean['text'] = df_clean['text'].apply(lambda x: x.lower())\n",
    "\n",
    "for punctuation in [',', '.', ':', ';', '?', '!', '\"', '/', '|', \"'\"]:\n",
    "    df_clean['text'] = df_clean['text'].apply(lambda x: x.replace(punctuation, ' '))\n",
    "\n",
    "for word in ['@realdonaldtrump', 'realdonaldtrump', 'rt', '&amp','trump', 'thank', 'people', 'great', 's']:\n",
    "\n",
    "    df_clean['text'] = df_clean['text'].apply(lambda x: x.replace(' '+word+' ', ' '))\n",
    "\n",
    "df_clean['text'] = df_clean['text'].apply(lambda x: x.replace('rt ', ''))\n",
    "df_clean['text'] = df_clean['text'].apply(lambda x: x.replace('thank', ''))\n",
    "df_clean['text'] = df_clean['text'].apply(lambda x: x.replace('president', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))\n",
    "\n",
    "def remove_stopwords(texts, spacy_stopwords):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in spacy_stopwords] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_clean['text'].values.tolist()\n",
    "data_words = list(sent_to_words(dataframe))\n",
    "data_words = remove_stopwords(data_words, spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = ' '.join([sentence for sublist in data_words for sentence in sublist])\n",
    "\n",
    "wordcloud = WordCloud(width=1500, height=800, random_state=21).generate(clean)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are not that much related to covid. Though one can already spot some of his favorite topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus our analysis on Covid-related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of words related to covid\n",
    "list_of_words = ['Covid19' ,'virus', 'vaccine', 'Flu', 'Covid', 'lockdown', 'pandemic', 'coronavirus', 'Coronavirus', 'Corona', 'corona', 'COVID19', 'COVID', 'covid', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantined', 'Quarantined', 'quarentined', 'Quarentined', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines']\n",
    "\n",
    "df_clean['covid_related'] = df_clean['text'].apply(lambda x: True if any(word in x for word in list_of_words) else False)\n",
    "\n",
    "df_clean['covid_related'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data where covid_related is true\n",
    "data_covid_rel = df_clean[df_clean['covid_related']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = data_covid_rel[(data_covid_rel['date']>=datetime(2020,3,8))&(data_covid_rel['date']<=datetime(2020,4,24))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same process, we can have a look at the most used words in Trump's Covid related tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data_covid_rel['text'].values.tolist()\n",
    "data_words = list(sent_to_words(dataframe))\n",
    "data_words = remove_stopwords(data_words, spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = ' '.join([sentence for sublist in data_words for sentence in sublist])\n",
    "\n",
    "wordcloud = WordCloud(width=1500, height=800, random_state=21).generate(clean)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows a more spreaded distribution of words. Though, one can spot the importance of china,vaccine ,chinese virus or even swine flu in his tweets. Lots of words were also related to actions such as Task Force, Live Press or Briefing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the moment when Trump tweeted the most about Covid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['month_date'] = df_clean.date.apply(lambda x: x.month)\n",
    "df_clean['year_date'] = df_clean.date.apply(lambda x: x.year)\n",
    "frequency_plot = df_clean.groupby(by=['month_date', 'year_date']).apply(lambda x: x.covid_related.sum() / x.covid_related.count()).reset_index()\n",
    "frequency_plot['date'] = [datetime(year=y, month=x, day=1) for x,y in zip(frequency_plot.month_date.values, frequency_plot.year_date.values)]\n",
    "frequency_plot = frequency_plot.sort_values(by='date')\n",
    "\n",
    "#plot the percentage of covid related tweets\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.plot(frequency_plot.date, frequency_plot.iloc[:,2] * 100)\n",
    "plt.title('Percentage of Donald Trump Covid-related Tweets (weekly)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the highest density of covid related tweets is in March 2020, which is the beginning of the pandemic in the US. In March the percentage peaked at 20%. We can also see that the density of covid related tweets is decreasing over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's discover if among the most retweeted and the most liked tweets there are some that are related to covid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_clean\n",
    "data['decile_retweeted'] = pd.qcut(data['retweets'], 10, labels=False, duplicates='drop')\n",
    "data['decile_favorites'] = pd.qcut(data['favorites'], 10, labels=False, duplicates='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all tweets have the same impact on public opinion, hence we want to see in which period covid related tweets were among the most relevant, to adress importance we use retweet as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot = data.groupby(by=['month_date', 'year_date']).apply(lambda x: \n",
    "                                                                    ((x.covid_related)*((x.decile_retweeted==9)|(x.decile_retweeted==8)|(x.decile_retweeted==7)|\n",
    "                                                                                        (x.decile_retweeted==6)|(x.decile_retweeted==5))).sum() \n",
    "                                                                                        / x.covid_related.count()).reset_index()\n",
    "frequency_plot['date'] = [datetime(year=y, month=x, day=1) for x,y in zip(frequency_plot.month_date.values, frequency_plot.year_date.values)]\n",
    "frequency_plot = frequency_plot.sort_values(by='date')\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.plot(frequency_plot.date, frequency_plot.iloc[:,2] * 100)\n",
    "plt.title('Percentage of Donald Trump Covid-related Tweets among the most retweeted (weekly)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see the moment were we had the highest share of covid related tweets (March 2020) is also the moment we had also the highest number of tweets related to covid.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobility analysis\n",
    "Show how during the pandemic, as people spent more time at home, the online traffic increase, there should be a direct correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing mobility reports, \n",
    "timeseries_dict = load_aggregated('./extra_material/wiki_pageviews_covid/data/aggregated_timeseries.json.gz')\n",
    "google_df = pd.read_csv('./extra_material/wiki_pageviews_covid/data/Global_Mobility_Report.csv.gz', compression=\"infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're only interested in the US, so we keep only the component about the US\n",
    "mobility_us_df = google_df[google_df.country_region_code == 'US'].drop(columns=['country_region', 'country_region_code']).copy()\n",
    "mobility_us_df['date'] = pd.to_datetime(mobility_us_df['date'])\n",
    "\n",
    "# We're interested in the complete and states values, so we can discard the the values for which sub_region_2 is not null\n",
    "mobility_us_df = mobility_us_df[mobility_us_df.sub_region_2.isna()].drop(columns=['sub_region_2','metro_area','iso_3166_2_code','census_fips_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df = mobility_us_df[(mobility_us_df.sub_region_1.isna()) & (mobility_us_df.date >= datetime(year=2020, month=2, day=15)) & (mobility_us_df.date <= datetime(year=2020, month=7, day=15))]\n",
    "general_df = general_df[['date', 'residential_percent_change_from_baseline']]\n",
    "\n",
    "weekday_general_df = general_df[general_df.date.dt.weekday <= 4].set_index('date')\n",
    "weekend_general_df = general_df[general_df.date.dt.weekday > 4].set_index('date')\n",
    "\n",
    "general_df = general_df.set_index('date')\n",
    "weekly_general_df = general_df.resample('W-Mon').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(15,5))\n",
    "fig.tight_layout(h_pad=5)\n",
    "\n",
    "ax.set_title('Time at home compared to previous years', size=25)\n",
    "ax.set_ylabel('Increase [%]', size=20)\n",
    "# ax.set_xlabel('Date', size=15)\n",
    "ax.plot(general_df)\n",
    "ax.plot(weekly_general_df)\n",
    "ax.scatter(weekday_general_df.index, weekday_general_df, color='red')\n",
    "ax.scatter(weekend_general_df.index, weekend_general_df, color='green')\n",
    "ax.legend(['Daily', 'Weekly', 'Weekday', 'Weekend'], fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting consideration, there is a difference between the change in mobility during the week (where people always go out for working) and during the weekend (where also normally people sometimes stay at home), this is clear looking at the daily plot, so it's beneficial to use the weekly one instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(15,5))\n",
    "fig.tight_layout(h_pad=5)\n",
    "\n",
    "ax.set_title('Daily Wikipedia usage from 2018 to 2020', size=25)\n",
    "ax.set_ylabel('Number of searches [millions]', size=20)\n",
    "# ax.set_xlabel('Date', size=15)\n",
    "ax.plot(timeseries_dict['en']['sum'] / 10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, it can be seen how the popularity of wikipedia is slightly decreasing over the years, we have to take this into account in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrease_factor = ((np.array(timeseries_dict['en']['sum'].loc[datetime(year=2018, month=2, day=17):datetime(year=2018, month=7, day=18)]) \\\n",
    "                   - np.array(timeseries_dict['en']['sum'].loc[datetime(year=2019, month=2, day=16):datetime(year=2019, month=7, day=17)])) \\\n",
    "                   / np.array(timeseries_dict['en']['sum'].loc[datetime(year=2019, month=2, day=16):datetime(year=2019, month=7, day=17)])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_searches_before = (np.array(timeseries_dict['en']['sum'].loc[datetime(year=2018, month=2, day=17):datetime(year=2018, month=7, day=18)]) \\\n",
    "                       + np.array(timeseries_dict['en']['sum'].loc[datetime(year=2019, month=2, day=16):datetime(year=2019, month=7, day=17)] * (1 + decrease_factor))) / 2\n",
    "total_searches_2020 = np.array(timeseries_dict['en']['sum'].loc[datetime(year=2020, month=2, day=15):datetime(year=2020, month=7, day=15)]) * (1 + decrease_factor)**2\n",
    "total_searches_increase = 100 * (total_searches_2020 / total_searches_before - 1)\n",
    "\n",
    "date_range = pd.date_range(datetime(year=2020, month=2, day=15), datetime(year=2020, month=7, day=15))\n",
    "\n",
    "total_searches_increase = pd.DataFrame({'date': date_range, 'increase': total_searches_increase})#.set_index('date')\n",
    "weekday_searches_increase = total_searches_increase[total_searches_increase.date.dt.weekday <= 4].set_index('date')\n",
    "weekend_searches_increase = total_searches_increase[total_searches_increase.date.dt.weekday > 4].set_index('date')\n",
    "\n",
    "total_searches_increase = total_searches_increase.set_index('date')\n",
    "weekly_total_searches_increase = total_searches_increase.resample('W-Mon').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(15,5))\n",
    "# fig.suptitle('Comparison between time spent at home and wikipedia searches between 2020 and previous years', size=20)\n",
    "fig.tight_layout(h_pad=5)\n",
    "\n",
    "ax.set_title('Wikipedia searches compared to previous years', size=25)\n",
    "ax.set_ylabel('Increase [%]', size=20)\n",
    "# ax.set_xlabel('Date', size=15)\n",
    "ax.plot(total_searches_increase)\n",
    "ax.plot(weekly_total_searches_increase)\n",
    "ax.scatter(weekday_searches_increase.index, weekday_searches_increase, color='red')\n",
    "ax.scatter(weekend_searches_increase.index, weekend_searches_increase, color='green')\n",
    "ax.legend(['Daily', 'Weekly', 'Weekday', 'Weekend'], fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot intuition is confirmed by applying a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting OLS for day, week, weekdays and weekends\n",
    "weekly_data = pd.DataFrame()\n",
    "weekly_data['x'] = weekly_general_df\n",
    "weekly_data['y'] = weekly_total_searches_increase\n",
    "\n",
    "daily_data = pd.DataFrame()\n",
    "daily_data['x'] = general_df\n",
    "daily_data['y'] = weekly_total_searches_increase\n",
    "\n",
    "weekday_data = pd.DataFrame()\n",
    "weekday_data['x'] = weekday_general_df\n",
    "weekday_data['y'] = weekday_searches_increase\n",
    "\n",
    "weekend_data = pd.DataFrame()\n",
    "weekend_data['x'] = weekend_general_df\n",
    "weekend_data['y'] = weekend_searches_increase\n",
    "\n",
    "# compute the different \n",
    "weekly_mod = smf.ols(formula='y ~ x', data=weekly_data)\n",
    "daily_mod = smf.ols(formula='y ~ x', data=daily_data)\n",
    "weekday_mod = smf.ols(formula='y ~ x', data=weekday_data)\n",
    "weekend_mod = smf.ols(formula='y ~ x', data=weekend_data)\n",
    "\n",
    "weekly_res = weekly_mod.fit()\n",
    "daily_res = daily_mod.fit()\n",
    "weekday_res = weekday_mod.fit()\n",
    "weekend_res = weekend_mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weekly_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing daily or weekly regressions give very similar results, both have good predictive power and similar coefficients (1.14 and 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weekday_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weekend_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, weekdays have a lower coefficient of 0.84, while weekends have a higher coefficient of 2.5192, this means that an increase in the time spent at home during the weekend is much more powerful than a weekday spent at home, for the number of Wikipedia searches done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Automatized Topic clustering using latent dirichlet allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we slice the dataset so that we we take into account only the most interesting period related to covid, which is from start of march to late may 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_period = df_clean[(df_clean['date']>'01-03-2020')&(df_clean['date']<'31-05-2020')]\n",
    "\n",
    "data = df_cov_period['text'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_words = remove_stopwords(data_words, spacy_stopwords)\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "num_topics = 3\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 6)) \n",
    "\n",
    "for topic_id, topic in enumerate(lda_model.print_topics(num_topics=num_topics, num_words=20)):\n",
    "    topic_words = \" \".join([word.split(\"*\")[1].strip() for word in topic[1].split(\" + \")])\n",
    "    wordcloud = WordCloud(width=300, height=400, random_state=21).generate(topic_words)\n",
    "\n",
    "    axs[topic_id].imshow(wordcloud)\n",
    "    axs[topic_id].axis(\"off\")\n",
    "    axs[topic_id].set_title(\"Topic: {}\".format(topic_id+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing an latent dirichlet analysis we can get some intuition of trump's most relevant topic of discussion. In topic 1 we can see clearly the more political oriented with words such as democrat, republicans ,biden, obama. In topic 3 there is a much more covid oriented cluster with keywords such as coronavirus, china, fake news.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Etienne's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#all tweets in one string\n",
    "result_text = ' '.join(data['text'])\n",
    "\n",
    "def remove_stopwords(chunk):\n",
    "    return ' '.join([token.text for token in chunk if (not token.is_stop)])\n",
    "\n",
    "#the text we have is too big to process it at once, so we split it into chunks\n",
    "chunk_size = 100000\n",
    "chunks = [result_text[i:i+chunk_size] for i in range(0, len(result_text), chunk_size)]\n",
    "processed_chunks = [nlp(chunk) for chunk in chunks]\n",
    "processed_chunks_no_stopwords = [remove_stopwords(chunk) for chunk in processed_chunks]\n",
    "\n",
    "#manually clean text from links, punctuation, etc.\n",
    "clean = ' '.join(processed_chunks_no_stopwords)\n",
    "clean= re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '',clean)#remove links\n",
    "clean= clean.translate(str.maketrans('', '', string.punctuation))#remove punctuation\n",
    "clean = clean.replace('amp', '')\n",
    "clean = clean.replace('RT', '')\n",
    "clean = clean.replace('realDonaldTrump', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scores'] = data['text'].apply(lambda Description: sid.polarity_scores(Description))  \n",
    "\n",
    "#calculate the sentiment score which is a combination of positive, negative and neutral(not mean)\n",
    "data['compound']  = data['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "data['sentiment_type']=''\n",
    "data.loc[data.compound>0,'sentiment_type']='POSITIVE'\n",
    "data.loc[data.compound==0,'sentiment_type']='NEUTRAL'\n",
    "data.loc[data.compound<0,'sentiment_type']='NEGATIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of words related to covid\n",
    "list_of_words = ['Covid19' ,'virus', 'vaccine', 'Flu', 'Covid', 'lockdown', 'pandemic', 'coronavirus', 'Coronavirus', 'Corona', 'corona', 'COVID19', 'COVID', 'covid', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantined', 'Quarantined', 'quarentined', 'Quarentined', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines', 'quarantine', 'Quarantine', 'quarentine', 'Quarentine', 'quarantining', 'Quarantining', 'quarentining', 'Quarentining', 'quarantines', 'Quarantines', 'quarentines', 'Quarentines']\n",
    "data['covid_related'] = data['text'].apply(lambda x: True if any(word in x for word in list_of_words) else False)\n",
    "\n",
    "data['covid_related'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data where covid_related is true\n",
    "data_covid_rel = data[data['covid_related']==1]\n",
    "data_not_covid_rel = data[data['covid_related']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-test between covid related and non related mean of retweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covid_rel = data[data['covid_related']==1]\n",
    "data_not_covid_rel = data[data['covid_related']==0]\n",
    "comparison_covid = pd.DataFrame({'Covid related':data_covid_rel.retweets, 'Non covid related':data_not_covid_rel.retweets})\n",
    "\n",
    "t_stat, p_value_covid = ttest_ind(data_covid_rel.retweets, data_not_covid_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_covid.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeakTheory related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_leaktheory = ['China virus', 'china virus', 'chinavirus', 'Chinavirus', 'China Virus']\n",
    "analyze_leaktheory = data['text'].apply(lambda x: True if any(word in x for word in fake_news_leaktheory) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_leaktheory.reindex(data.index, fill_value=False)\n",
    "data['Leaktheory'] = new_col\n",
    "print('Number of tweets talking about COVID-19 lab leak theory is: ', data['Leaktheory'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Leaktheory_rel = data[data['Leaktheory']==1]\n",
    "data_not_Leaktheory_rel = data[data['Leaktheory']==0]\n",
    "comparison_Leaktheory = pd.DataFrame({'Leaktheory related':data_Leaktheory_rel.retweets, 'Non Leaktheory related':data_not_Leaktheory_rel.retweets})\n",
    "\n",
    "t_stat, p_value_Leaktheory = ttest_ind(data_Leaktheory_rel.retweets, data_not_Leaktheory_rel.retweets, equal_var=False)\n",
    "print('The p-value is', p_value_Leaktheory.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swine flu related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_swineflu = ['Swine', 'swine', 'H1N1', 'h1n1']\n",
    "analyze_swineflu = data['text'].apply(lambda x: True if any(word in x for word in fake_news_swineflu) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_swineflu.reindex(data.index, fill_value=False)\n",
    "data['Swineflu'] = new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Swineflu_rel = data[data['Swineflu']==1]\n",
    "data_not_Swineflu_rel = data[data['Swineflu']==0]\n",
    "comparison_Swineflu = pd.DataFrame({'Swineflu related':data_Swineflu_rel.retweets, 'Non Swineflu related':data_not_Swineflu_rel.retweets})\n",
    "\n",
    "t_stat, p_value_Swineflu = ttest_ind(data_Swineflu_rel.retweets, data_not_Swineflu_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_Swineflu.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydroxychloroquine related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = ['Hydroxychloroquine', 'hydroxychloroquine']\n",
    "analyze = data['text'].apply(lambda x: True if any(word in x for word in fake_news) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze.reindex(data.index, fill_value=False)\n",
    "data['Hydroxychloroquine']=new_col\n",
    "\n",
    "print('Number of tweets talking about Hydroxychloroquine is: ', data['Hydroxychloroquine'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Hydroxychloroquine_rel = data[data['Hydroxychloroquine']==1]\n",
    "data_not_Hydroxychloroquine_rel = data[data['Hydroxychloroquine']==0]\n",
    "comparison_Hydroxychloroquine = pd.DataFrame({'Hydroxychloroquine related':data_Hydroxychloroquine_rel.retweets, 'Non Hydroxychloroquine related':data_not_Hydroxychloroquine_rel.retweets})\n",
    "\n",
    "t_stat, p_value_Hydroxychloroquine = ttest_ind(data_Hydroxychloroquine_rel.retweets, data_not_Hydroxychloroquine_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_Hydroxychloroquine.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biden related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden = ['Joe Biden','biden','joe','Joe','Biden','sleepy']\n",
    "analyze_biden = data['text'].apply(lambda x: True if any(word in x for word in biden) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_biden.reindex(data.index, fill_value=False)\n",
    "data['biden'] = new_col\n",
    "print('Number of tweets talking about biden is: ', data['biden'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_biden_rel = data[data['biden']==1]\n",
    "data_not_biden_rel = data[data['biden']==0]\n",
    "comparison_biden = pd.DataFrame({'biden related':data_biden_rel.retweets, 'Non biden related':data_not_biden_rel.retweets})\n",
    "\n",
    "t_stat, p_value_biden = ttest_ind(data_biden_rel.retweets, data_not_biden_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_biden.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Democrats related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "democrats = ['Democrats','democrats','Dems','dems']\n",
    "analyze_democrats = data['text'].apply(lambda x: True if any(word in x for word in democrats) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_democrats.reindex(data.index, fill_value=False)\n",
    "data['democrats'] = new_col\n",
    "print('Number of tweets talking about democrats is: ', data['democrats'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_democrats_rel = data[data['democrats']==1]\n",
    "data_not_democrats_rel = data[data['democrats']==0]\n",
    "comparison_democrats = pd.DataFrame({'democrats related':data_democrats_rel.retweets, 'Non democrats related':data_not_democrats_rel.retweets})\n",
    "\n",
    "t_stat, p_value_democrats = ttest_ind(data_democrats_rel.retweets, data_not_democrats_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_democrats.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vaccines related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccines = ['vaccines','Vaccines','vax','Vax','vaccin']\n",
    "analyze_vaccines = data['text'].apply(lambda x: True if any(word in x for word in democrats) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_vaccines.reindex(data.index, fill_value=False)\n",
    "data['vaccines'] = new_col\n",
    "print('Number of tweets talking about vaccines of the t-test is: ', data['vaccines'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vaccines_rel = data[data['vaccines']==1]\n",
    "data_not_vaccines_rel = data[data['vaccines']==0]\n",
    "comparison_vaccines = pd.DataFrame({'vaccines related':data_vaccines_rel.retweets, 'Non vaccines related':data_not_vaccines_rel.retweets})\n",
    "\n",
    "\n",
    "t_stat, p_value_vaccines = ttest_ind(data_vaccines_rel.retweets, data_not_vaccines_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_vaccines.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate results into more general categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_fake_news'] = (data['Hydroxychloroquine']|data['Swineflu']|data['Leaktheory'])\n",
    "data['is_democrats_related'] = (data['biden']|data['democrats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_democrats_related'] .sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake_news_rel = data[data['is_fake_news']==1]\n",
    "data_not_fake_news_rel = data[data['is_fake_news']==0]\n",
    "comparison_fake_news = pd.DataFrame({'fake_news related':data_fake_news_rel.retweets, 'Non fake_news related':data_not_fake_news_rel.retweets})\n",
    "\n",
    "\n",
    "t_stat, p_value_fake_news = ttest_ind(data_fake_news_rel.retweets, data_not_fake_news_rel.retweets, equal_var=False)\n",
    "print('The p-value of the t-test is', p_value_fake_news.round(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of number of rewteets by topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 2,figsize=(30,8))\n",
    "\n",
    "ax0 = sns.pointplot(data=comparison_covid, ax=axs[0, 0], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax0 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax0 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax1 = sns.pointplot(data=comparison_Leaktheory,ax=axs[0, 1], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax1 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax1 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax2 = sns.pointplot(data=comparison_Swineflu, ax=axs[1, 0], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax2 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax2 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax3 = sns.pointplot(data=comparison_Hydroxychloroquine, ax= axs[1, 1], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax3 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax3 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax4 = sns.pointplot(data=comparison_biden,ax=axs[2, 0], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax4 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax4 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax5 = sns.pointplot(data=comparison_democrats, ax=axs[2, 1], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax5 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax5 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax6 = sns.pointplot(data=comparison_vaccines, ax=axs[3, 0], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax6 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax6 .set(ylabel='Number of retweets')\n",
    "\n",
    "ax7 = sns.pointplot(data=comparison_fake_news, ax=axs[3, 1], estimator='mean', errorbar=('ci', 95), color = 'r')\n",
    "ax7 .set(title='Comparison of retweets (errorbar = CI 95%)')\n",
    "ax7 .set(ylabel='Number of retweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression to disentangle categories into predicting the number of retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mod_retweets = smf.ols(formula='retweets ~ C(covid_related)+C(is_fake_news)+C(sentiment_type)', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_retweets = mod_retweets.fit()\n",
    "print(res_retweets.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare number of retweets per categories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:20:28.187086Z",
     "start_time": "2023-12-21T12:20:28.171582Z"
    }
   },
   "outputs": [],
   "source": [
    "compare_retweets = pd.DataFrame({'democrats':data_democrats_rel.retweets,'biden':data_biden_rel.retweets,\n",
    "                    'covid':data_covid_rel.retweets,'Swine flu':data_Swineflu_rel.retweets,\n",
    "                     'Leak Theory':data_Leaktheory_rel.retweets,'Hydroxychloroquine':data_Hydroxychloroquine_rel.retweets,\n",
    "                               'vaccines':data_vaccines_rel.retweets})\n",
    "\n",
    "ax = sns.barplot(compare_retweets)\n",
    "ax.set(xlabel='Categories', ylabel='Number of rewtweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add one hot encoding of sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =data.merge(pd.get_dummies(data['sentiment_type']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=False, figsize=(4, 4))\n",
    "\n",
    "var = 'sentiment_type'\n",
    "colors = ['#FFCCCC', 'lightgreen','green']\n",
    "print(data[var].value_counts())\n",
    "plt.pie(data[var].value_counts(), labels=['Positive', 'Negative','Neutral'], autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "plt.title(f'Distribution of sentiments')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DT is actually a positive guy!**\n",
    "\n",
    "Show a few tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.sentiment_type=='POSITIVE'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.sentiment_type=='NEUTRAL'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.sentiment_type=='NEGATIVE'].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump's sentiment is predictable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a function that predicts sentiment based on a subset of a Tweet's dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_(df):\n",
    "  #get the scores of each tweet\n",
    "  df['scores'] = df['text'].apply(lambda Description: sid.polarity_scores(Description)) \n",
    "\n",
    "  #calculate the sentiment score which is a combination of positive, negative and neutral(not mean)\n",
    "  df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "  df['sentiment_type']=''\n",
    "  df.loc[df.compound>0,'sentiment_type']='POSITIVE'\n",
    "  df.loc[df.compound==0,'sentiment_type']='NEUTRAL'\n",
    "  df.loc[df.compound<0,'sentiment_type']='NEGATIVE'\n",
    "\n",
    "  #aggrergate by day month and year # we want to inspect aggregate value per day, week and month\n",
    "  df['month_date'] = df.date.apply(lambda x: x.month)\n",
    "  df['year_date'] = df.date.apply(lambda x: x.year)\n",
    "  df['daily'] = df.date.apply(lambda x: x.day)\n",
    "  df['week'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "  # take the mean of the compound value for each day, week and month\n",
    "  #daily\n",
    "  freq_d = df.groupby(by=['year_date','month_date','daily']).apply(lambda x: x.compound.mean())\n",
    "  freq_d = freq_d.reset_index()\n",
    "  freq_d = freq_d.rename(columns={0: 'compound'})\n",
    "  \n",
    "  #weekly\n",
    "  freq_w = df.groupby(['year_date','month_date','week']).apply(lambda x: x.compound.mean())\n",
    "  freq_w = freq_w.reset_index()\n",
    "  freq_w = freq_w.rename(columns={0: 'compound'})\n",
    "\n",
    "  #monthly\n",
    "  freq_m = df.groupby(by=['year_date','month_date']).apply(lambda x: x.compound.mean())\n",
    "  freq_m = freq_m.reset_index()\n",
    "  freq_m = freq_m.rename(columns={0: 'compound'})\n",
    "\n",
    "  # drop week first last two weeks unresonable values\n",
    "  freq_w = freq_w.iloc[1:-1]\n",
    "\n",
    "  #monthly\n",
    "  freq_m.year_date = freq_m.year_date.astype(str)\n",
    "  freq_m.month_date = freq_m.month_date.astype(str)\n",
    "  freq_m['date'] = freq_m.apply(lambda x: datetime.strptime(str(x.year_date) + '-' + str(x.month_date), '%Y-%m'), axis=1)\n",
    "\n",
    "  #manipulation to get nice plot\n",
    "  #daily\n",
    "  freq_d.year_date = freq_d.year_date.astype(str)\n",
    "  freq_d.month_date = freq_d.month_date.astype(str)\n",
    "  freq_d.daily = freq_d.daily.astype(str)\n",
    "  freq_d['date'] = freq_d.apply(lambda x: datetime.strptime(str(x.year_date) + '-' + str(x.month_date) + '-' + str(x.daily), '%Y-%m-%d'), axis=1)\n",
    "\n",
    "  #weekly\n",
    "  freq_w.year_date = freq_w.year_date.astype(str)\n",
    "  freq_w.month_date = freq_w.month_date.astype(str)\n",
    "  freq_w.week = freq_w.week.astype(str)\n",
    "  freq_w['date'] = pd.to_datetime(freq_w['year_date'].astype(str) + freq_w['week'].astype(str) + '-1', format='%G%V-%u')\n",
    "  return df, freq_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tot, freq_d = get_sentiment_(data_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid Related Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data where covid_related is true\n",
    "data_t['covid_related'] = data_t['text'].apply(lambda x: True if any(word in x for word in list_of_words) else False)\n",
    "data_covid_rel = data_t[data_t['covid_related']==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drr, freq_dc = get_sentiment_(data_covid_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First variable: sentiment in the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if we find predictors for Trump's tweets sentiment.\n",
    "From kaggle datset 20 March - 18 April, data from kaggle sentiment among people in US.<p>\n",
    "The Idea is to see if sentiment in population has predictive power on Trump's tweets sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset old one\n",
    "start_date = '2020-03-19'\n",
    "end_date = '2020-04-18'\n",
    "freq_d_subc = freq_dc[(freq_dc['date'] >= start_date) & (freq_dc['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "combined_df = pd.DataFrame()\n",
    "file_path = 'dati'\n",
    "files = [file for file in os.listdir(file_path)]\n",
    "# Create a list of DataFrames by reading each CSV file\n",
    "dataframes = [pd.read_csv(file_path+'/'+file) for file in files]\n",
    "\n",
    "# Concatenate the list of DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df= combined_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column date combining year, month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.user_location.value_counts()\n",
    "US_df = combined_df[combined_df['user_location']=='United States']\n",
    "US_df['date']=pd.to_datetime(US_df['created_at'], format='%a %b %d %H:%M:%S +0000 %Y', errors='coerce').dt.floor('D') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_df.loc[:,'scores'] = US_df['text'].apply(lambda review: sid.polarity_scores(review))\n",
    "\n",
    "US_df['compound']  = US_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "#consider the one different from 0\n",
    "US_df=US_df[US_df['compound']!=0]\n",
    "#aggregate vale per date\n",
    "US_df_agg = US_df['compound'].groupby(US_df['date']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(US_df_agg['date'], US_df_agg['compound'], label='people', linestyle='-')\n",
    "plt.plot(freq_d_subc['date'], freq_d_subc['compound'], label='trump', linestyle='-')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Compound Value')\n",
    "plt.title('Compound Values Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge dataframes with people sentiment and trump sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfc = pd.merge(US_df_agg, freq_d_subc, on='date', how='inner')\n",
    "merged_dfc.rename(columns={'compound_x': 'people', 'compound_y': 'Trump'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataset with only tweets about covid, let's see if there is a correlation between the sentiment of the people about covid and the sentiment of Trump's tweets(only about covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfc['Trump_sent']= merged_dfc['Trump'].apply(lambda x: 1 if x>0 else 0 )\n",
    "merged_dfc['people_sent']= merged_dfc['people'].apply(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_1 = smf.logit(formula='Trump_sent ~ people_sent ', data=merged_dfc)\n",
    "res_1 = mod_1.fit()\n",
    "print(res_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second variable : New infected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a timeseries of new cases of covid in US and see if the variation of the new cases can be used to predict Trump's sentiment tweets the dataset reports the absolute number of new cases, we need to calculate the variation of new cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_csv('timesco/time-series-19-covid-combined.csv')\n",
    "covid.head()\n",
    "\n",
    "cov_us = covid[covid['Country/Region']=='US']\n",
    "sub_cov = cov_us[(cov_us['Date'] >= start_date) & (cov_us['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the columns with people and Trump sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cov.loc[:,'trump_sent'] = merged_dfc['Trump_sent'].values\n",
    "sub_cov.loc[:,'peopl_sent'] = merged_dfc['people_sent'].values\n",
    "sub_cov.loc[:,'Trump']= merged_dfc['Trump'].values\n",
    "sub_cov.loc[:,'people']= merged_dfc['people'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variation of cases in new cases, in the dataframe we have absolute numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increment in new cases\n",
    "logit_df = sub_cov.copy()\n",
    "logit_df['increase_new_cases'] = logit_df['Confirmed'].shift(1)-logit_df['Confirmed']/logit_df['Confirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization\n",
    "logit_df['Confirmed']= (logit_df['Confirmed']-logit_df['Confirmed'].mean())/logit_df['Confirmed'].std()\n",
    "logit_df['Recovered']= (logit_df['Recovered']-logit_df['Recovered'].mean())/logit_df['Recovered'].std()\n",
    "logit_df['Deaths']= (logit_df['Deaths']-logit_df['Deaths'].mean())/logit_df['Deaths'].std()\n",
    "logit_df['increase_new_cases']= (logit_df['increase_new_cases']-logit_df['increase_new_cases'].mean())/logit_df['increase_new_cases'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression\n",
    "mod5 = smf.logit(formula='trump_sent ~ increase_new_cases ', data=logit_df)\n",
    "res5 = mod5.fit()\n",
    "print(res5.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary we can see that the variation of new cases has a negative correlation with Trump's sentiment tweets. It is significative at 5% level, then we have predictive power.\n",
    "It is reasonable since we expect that if the number of new cases is increasing, Trump's sentiment is worsening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine variables : sentiment and new cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod6 = smf.logit(formula='trump_sent ~ peopl_sent + increase_new_cases', data=logit_df)\n",
    "res6 = mod6.fit()\n",
    "print(res6.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results we got in the previous models are confirmed, the two variables have predictive power and the model performs well. Actually the p-values are ven lower than in the previous models, both variables are significative at 2.5% level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that both variables have predictive power, let's scale the increment in new cases between -1 and 1 with min max scaler, so that they are directly comparable to the sentiment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is the variable you want to scale\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled = logit_df['increase_new_cases'].values.reshape(-1, 1)  # If 'data' is a pandas Series\n",
    "logit_df['scaled_new'] = scaler.fit_transform(scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "plt.plot(logit_df['Date'], logit_df['scaled_new'], label='increase_new_cases', linestyle='-',color='green')\n",
    "plt.plot(logit_df['Date'], logit_df['Trump'], label='Trump_sent', linestyle='-',color='red')\n",
    "plt.plot(logit_df['Date'], logit_df['people'], label='people_sent', linestyle='-', color='blue')\n",
    "\n",
    "n = 7 \n",
    "plt.xticks(logit_df['Date'][::n])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Compound Value')\n",
    "plt.title('Compound Values Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot show that the two sentiment variables comove, at least is the period considered. It also noticible that the sentiments variables are negatively correlated with the variation of new cases, as expected. The trend of the variation of new cases is raising sharply, wherease the sentiment of the sentiment variables have a general negative trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = res6.predict(logit_df[['peopl_sent', 'increase_new_cases']])\n",
    "y_pred = [ 1 if y>=0.5 else 0 for y in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(logit_df.trump_sent, y_pred)\n",
    "labels = ['False', 'True']  \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='viridis', xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 20}, cbar=False)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "recall = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print('Precision: {:.3f}'.format(precision))\n",
    "print('Recall: {:.3f}'.format(recall))\n",
    "print('F1 Score: {:.3f}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we get are really good we have a precision of 0.8 and a recall of 0.8, the model is able to predict the sentiment of Trump's tweets with a good accuracy and recall. Our analysis last 30 days but the values are aggregated on a daily basis so our results can be considered reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the miss values given with the mean value\n",
    "logit_df['increase_new_cases']=logit_df['increase_new_cases'].fillna(logit_df['increase_new_cases'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_probs = res6.predict(logit_df[['peopl_sent', 'increase_new_cases']])\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "logit_roc_auc = roc_auc_score(logit_df['trump_sent'], logit_probs)\n",
    "fpr, tpr, thresholds = roc_curve(logit_df['trump_sent'], logit_probs)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (AUC = %0.2f)' % logit_roc_auc, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"AUC = {:.3f}\".format(auc_score))\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Retweets per sentiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.pointplot(data,x='sentiment_type',y='retweets').set(title='Number of retweets per sentiment category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study relation between categories and sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pivot_dem = pd.crosstab(data[data['democrats']==True].democrats, data['sentiment_type'])\n",
    "pivot_biden = pd.crosstab(data[data['biden']==True].biden, data['sentiment_type'])\n",
    "pivot_covid = pd.crosstab(data[data['covid_related']==True].covid_related, data['sentiment_type'])\n",
    "pivot_fake_news = pd.crosstab(data[data['is_fake_news']==True].is_fake_news, data['sentiment_type'])\n",
    "pivot_vaccines = pd.crosstab(data[data['vaccines']==True].vaccines, data['sentiment_type'])\n",
    "\n",
    "pivot_df = pd.concat([pivot_dem,pivot_biden,pivot_covid,pivot_fake_news,pivot_vaccines])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "proportion_df = 100*pivot_df.div(pivot_df.sum(axis=1), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,12))\n",
    "ax = proportion_df.plot(kind='bar', stacked=True, colormap='viridis', figsize=(10, 6))\n",
    "plt.title(\"Stacked Bar plot of sentiment's proportion for each topic\")\n",
    "ax.set_xticklabels(['Democrats related','Biden related','Covid related','Fake news related','Vaccines related'])\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Sentiment Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweets per sentiment for given categories ( to delete I guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution: some 'positive' are actually sarcastic:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_biden_rel[data_biden_rel.sentiment_type=='POSITIVE'].text[1720]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Causal Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "from datetime import datetime, timedelta\n",
    "from causalimpact import CausalImpact\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# helpers\n",
    "from helpers.wiki_gtrend_visualization import load_wikipedia_df\n",
    "from helpers.gtrend_visualization import load_gtrend_df, load_gtrend_hourly_df\n",
    "from helpers.load_topic_hourly import request_gtrends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our research questions is to find whether there is a causal relationship between Trump's tweets and the number of visits on some Covid-related topics, for example Hydroxychloroquine on Wikipedia. During COVID-19 period Trump claimed that Hydroxychloroquine was a cure for COVID-19. This claim was not supported by scientific evidence, making it more of a fake news. This was further motivation to investigate this topic in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('data/tweets.csv', parse_dates=['date'], date_format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains all his tweets and retweets from 2009 to January, 8 2021 (one day before he was banned from Twitter) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.loc[1, 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the Covid period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_covid_df = tweets_df[tweets_df['date'] >= '2019-12-01'].copy()\n",
    "tweets_covid_df.sort_values(by='date')\n",
    "tweets_covid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Wikipedia & GTrends Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of tweets where Hydroxychloroquine is mentioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinpoint all tweets containing the key words\n",
    "fake_news = ['Hydroxychloroquine', 'hydroxychloroquine']\n",
    "analyze = tweets_covid_df['text'].apply(lambda x: True if any(word in x for word in fake_news) else False)\n",
    "\n",
    "# Create a column with a boolean indicating whether tweet contains a key word\n",
    "new_col = analyze.reindex(tweets_covid_df.index, fill_value=False)\n",
    "tweets_covid_df['Hydroxychloroquine']=new_col\n",
    "\n",
    "print('Number of tweets talking about Hydroxychloroquine is: ', tweets_covid_df['Hydroxychloroquine'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dates where trump spoke about Hydroxychloroquine\n",
    "hydro_tweets_times = tweets_covid_df[tweets_covid_df['Hydroxychloroquine'] == True]['date'].copy()\n",
    "hydro_tweets_times = hydro_tweets_times.sort_values()\n",
    "hydro_tweets_times = hydro_tweets_times.reset_index(drop=True)\n",
    "hydro_tweets_times = hydro_tweets_times.dt.normalize() # set times to 00:00:00\n",
    "display(hydro_tweets_times.head())\n",
    "week_critical_data_hydro = (hydro_tweets_times - pd.to_timedelta((hydro_tweets_times.dt.dayofweek+1) % 7, unit='D')).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the global number of pageviews for the article related to hydroxychloroquine from Wikipedia and Google Trends. We will start by graphically studying the evolution of the number of queries/visits to assess whether Trump's tweets had an impact on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagename = 'Hydroxychloroquine'\n",
    "folder = 'extra_material'\n",
    "filename = 'hydroxychloroquine'\n",
    "\n",
    "weekly_wikipedia_hydro_df, daily_wikipedia_hydro_df = load_wikipedia_df(folder,filename)\n",
    "gtrends_hydro_df = load_gtrend_df(folder, filename)\n",
    "\n",
    "daily_wikipedia_hydro_df = daily_wikipedia_hydro_df.drop('Week', axis=1)\n",
    "daily_wikipedia_hydro_df\n",
    "\n",
    "fig, axis = plt.subplots(ncols=1, nrows=2, figsize=(10,5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "fig.suptitle('Weekly searches related to {pagename}'.format(pagename=pagename), size=20)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "axis[0].set_title('Google Trends', size=15)\n",
    "axis[0].set_ylabel('Relative search interest')\n",
    "axis[0].plot(gtrends_hydro_df['Views'])\n",
    "[axis[0].axvline(x=critical_date, color='red', linestyle='--') for critical_date in week_critical_data_hydro]\n",
    "\n",
    "axis[1].set_title('Wikipedia', size=15)\n",
    "axis[1].set_ylabel('Number of views [k]')\n",
    "axis[1].plot(weekly_wikipedia_hydro_df['Views']/10**3)\n",
    "[axis[1].axvline(x=critical_date, color='red', linestyle='--') for critical_date in week_critical_data_hydro]\n",
    "axis[1].set_xlabel('Date')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Wikipedia and Google trend pageviews behave simarly. Google Trends uses a relative scale from 0 to 100, while Wikipedia returns the absolute number of visits. We can see that the number of visits was almost zero until March 2020. The spikes in the graph appear almost at the same time as Trump's Tweet. We should further investigate whether there is a causal relationship between Trump's tweets and the number of visits.\n",
    "\n",
    "To get a finer visualization of the impact of Trump's tweets on Wikipedia searches, we can also plot Wikipedia searches on hydroxychloroquine at a daily granularity (this granularity is not available on Google Trends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "\n",
    "plt.title('Daily Wikipedia searches related to {pagename}'.format(pagename=pagename))\n",
    "plt.ylabel('Number of views [k]')\n",
    "plt.plot(daily_wikipedia_hydro_df['Views']/10**3)\n",
    "[plt.axvline(x=critical_date, color='red', linestyle='--') for critical_date in hydro_tweets_times]\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph above, Trump's tweets on hydroxychloroquine seem to precede certain attention peaks (e.g. the second big peak at the start of April), but come after others (e.g. the first big peak, around mid-March). A possibile explanation is simply that the tweets sometimes cause attention, whereas other times public interests makes Trump tweet about the topic. Another one is that both are caused by external factors, and that Trump's and the public's reaction times vary: sometimes Trump reacts the fastest, and other times the public does.\n",
    "\n",
    "As hydroxychloroquine  was not the only fake news spread by Trump, we repeated the same preliminary analysis and visualizations for other fake news, to get a better idea of how well our observations for hydroxychloroquine generalize to other topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)  Swine Flu\n",
    "\n",
    "Trump tweeted a lot about the Swine Flu. Let's graphically study the relationship between Trump's tweets and public interest for the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_swineflu = ['Swine', 'swine', 'H1N1', 'h1n1']\n",
    "analyze_swineflu = data['text'].apply(lambda x: True if any(word in x for word in fake_news_swineflu) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_swineflu.reindex(data.index, fill_value=False)\n",
    "data['Swineflu'] = new_col\n",
    "print('Number of tweets talking about Swine Flu is: ', data['Swineflu'].sum())\n",
    "\n",
    "# get the dates where trump spoke\n",
    "critical_data = data[data['Swineflu'] == True]['date']\n",
    "week_critical_data = (critical_data - pd.to_timedelta((critical_data.dt.dayofweek+1) % 7, unit='D')).unique()\n",
    "            \n",
    "pagename = 'Swine Influenza'\n",
    "filename = 'swineflu'\n",
    "\n",
    "weekly_wikipedia_df, _ = load_wikipedia_df(folder, filename)\n",
    "gtrends_df = load_gtrend_df(folder, filename)\n",
    "\n",
    "fig, axis = plt.subplots(ncols=1, nrows=2, figsize=(10,5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "fig.suptitle('Weekly searches related to {pagename}'.format(pagename=pagename), size=20)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "axis[0].set_title('Google Trends', size=15)\n",
    "axis[0].set_ylabel('Relative search interest')\n",
    "axis[0].plot(gtrends_df['Views'])\n",
    "[axis[0].axvline(x=critical_date, color='red', linestyle='--') for critical_date in week_critical_data]\n",
    "\n",
    "axis[1].set_title('Wikipedia', size=15)\n",
    "axis[1].set_ylabel('Number of views [k]')\n",
    "axis[1].plot(weekly_wikipedia_df['Views']/10**3)\n",
    "[axis[1].axvline(x=critical_date, color='red', linestyle='--') for critical_date in week_critical_data]\n",
    "axis[1].set_xlabel('Date')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it seems that peaks in popularity tend to start before Trump's tweets (especially the first two), but some of Trump's tweets might still have had some effect. His first tweet seemed to have increased the slope of the main popularity peak, and his tweets in mid-june 2020 might have cause the subsequent peak in interest (a smaller one, but Trump's tweet clearly precedes it this time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)  China Laboratory-Leak Virus\n",
    "\n",
    "Trump was not tweeting directly about the fact that covid escaped from a lab, however, he tweeted a lot about the \"China Virus\", did this feed the trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_leaktheory = ['China virus', 'china virus', 'chinavirus', 'Chinavirus', 'China Virus']\n",
    "analyze_leaktheory = data['text'].apply(lambda x: True if any(word in x for word in fake_news_leaktheory) else False)\n",
    "\n",
    "# create a new column with the result of the analysis\n",
    "new_col = analyze_leaktheory.reindex(data.index, fill_value=False)\n",
    "data['Leaktheory'] = new_col\n",
    "print('Number of tweets talking about COVID-19 lab leak theory is: ', data['Leaktheory'].sum())\n",
    "\n",
    "# get the dates where trump spoke\n",
    "critical_data = data[data['Leaktheory'] == True]['date']\n",
    "week_critical_data = (critical_data - pd.to_timedelta((critical_data.dt.dayofweek+1) % 7, unit='D')).unique()\n",
    "            \n",
    "pagename = 'COVID-19 lab leak theory'\n",
    "filename = 'leaktheory'\n",
    "\n",
    "weekly_wikipedia_df, _ = load_wikipedia_df(folder, filename)\n",
    "gtrends_df = load_gtrend_df(folder, filename)\n",
    "\n",
    "plt.figure(figsize=(8, 4)) \n",
    "\n",
    "plt.suptitle('Weekly searches related to {pagename} on Google Trends'.format(pagename=pagename), size=20)\n",
    "\n",
    "plt.ylabel('Relative search interest')\n",
    "plt.plot(gtrends_df['Views'])\n",
    "[plt.axvline(x=critical_date, color='red', linestyle='--') for critical_date in week_critical_data]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'Lab Leak Theory' the graph is much more hectic and it's hard to conclude on any causal effect of Trump's tweets on online trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Granger Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand Trump's potential causal impact on online trends, we will focus on the topic of hydroxychloroquine. \n",
    "The plot of daily Wikipedia views made it difficult to assess whether Trump causes tweets or the other way around. To investigate this, we therefore chose to conduct a [Granger causality](https://en.wikipedia.org/wiki/Granger_causality) test. \n",
    "\n",
    "\"The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another\".\n",
    "The [null hypothesis](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.grangercausalitytests.html) is that \"the the time series the first column is NOT Granger caused by the time series in the other columns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the tweets in a format compatible with Granger causality analysis (i.e. time series)\n",
    "tweets_hydro_timeseries_df = pd.DataFrame(index=daily_wikipedia_hydro_df['Views'].index, columns=['Date'])\n",
    "\n",
    "for date in hydro_tweets_times:\n",
    "    tweets_hydro_timeseries_df[date] = (tweets_hydro_timeseries_df.index == date).astype(int)\n",
    "\n",
    "tweets_hydro_timeseries_df['Date'].fillna(0, inplace=True)\n",
    "tweets_hydro_timeseries_df['Tweet'] = tweets_hydro_timeseries_df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Combine wiki and tweets\n",
    "wiki_tweets_hydro_df = daily_wikipedia_hydro_df.copy()\n",
    "wiki_tweets_hydro_df['Tweets'] = tweets_hydro_timeseries_df['Tweet']\n",
    "wiki_tweets_hydro_df['Tweets'].fillna(0, inplace=True)\n",
    "display(wiki_tweets_hydro_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's investigate whether tweets Granger cause views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n DO TWEETS GRANGER CAUSE VIEWS? \\n\")\n",
    "tweets_cause_views = grangercausalitytests(wiki_tweets_hydro_df[['Tweets', 'Views']], 2, verbose=False)\n",
    "\n",
    "print(f\"p-value, 1 lag: {tweets_cause_views[1][0]['ssr_ftest'][1]}\")\n",
    "print(f\"p-value, 2 lags: {tweets_cause_views[2][0]['ssr_ftest'][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values for the Granger causality tests for both lags 1 and 2 are well below the threshold of 0.05. The test is therefore statistically signifcant and the views time series is Granger caused by tweets.\n",
    "\n",
    "\n",
    "Let's repeat our analysis the other way around: do views Granger cause Trump's tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DO VIEWS GRANGER CAUSE TWEETS?\")\n",
    "views_cause_tweets = grangercausalitytests(wiki_tweets_hydro_df, 2, verbose=False)\n",
    "\n",
    "print(f\"p-value, 1 lag: {views_cause_tweets[1][0]['ssr_ftest'][1]}\")\n",
    "print(f\"p-value, 2 lags: {views_cause_tweets[2][0]['ssr_ftest'][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, only the p-value for 2 lags is below the statistical threshold of significance of 0.05. According to [Wikipedia](https://en.wikipedia.org/wiki/Granger_causality), however, \"the null hypothesis of no Granger causality is not rejected if and only if no lagged values of an explanatory variable have been retained in the regression\". In other words, having signifance for one lagged value is enough. We can thus conclude that views also cause Trump's tweets.\n",
    "\n",
    "How to interpret the fact that the two time series Granger cause each other? As mentioned earlier, Granger causality just means that one time series is useful at predicting the other. There can therefore be two explanations to our results:\n",
    "- sometimes Trump causes tweets, other times public interest causes Trump to tweet;\n",
    "- or both time series are actually caused by external factors. Sometimes Trump's reacts faster, and sometimes the public get interested first. The varying reaction times could explain the Granger causality results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Causal Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further investigate whether Trump's tweet cause views, we will focus on his first tweet, which coincides with a big peak in interest (both on Wikipedia and Google).\n",
    "The [Causal Impact](https://google.github.io/CausalImpact/CausalImpact.html) library in Python allows us perform a test which can tell us if there is a causal relation between Trump's first tweet and the number of Google queries on Hydroxychloroquine. We are focusing on Google Trends, as they allows us to study time series at hourly granularity. \n",
    "To run this analysis we need to build a dataframe with the following columns:\n",
    "- data index: the date of the observation of our time series.\n",
    "- y: the number of visits to the the page of Hydroxychloroquine (test variable).\n",
    "- x: the number of visits to a set of pages (the control variables), which were not affected by the intervention (Trump's first tweet).\n",
    "\n",
    "The following [assumptions](https://pypi.org/project/pycausalimpact/) need to be verified to conduct this test : \"the response variable can be precisely modeled by a linear regression with what is known as \"covariates\" (or X) that must not be affected by the intervention that took place\".\n",
    "\n",
    "We decided to take the time series of the following five topics, which for obvious reasons should not have been affected by Trump's tweet, as control variables: climate, coffee, news, shop and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Fetch GTrends Time Series at Hourly Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fetching the time series of interest from Google Trends. We focus on a window of 4 days centred around Trump's first tweet. Note that we are using the time series of global searches on hydroxychloroquine. Unfortunately, more local series were not available for the data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tweet number\n",
    "tweet_nb = 0\n",
    "alias = f\"tweet_{tweet_nb}\"\n",
    "\n",
    "# Request data from GTrends API (both hydro and Google) and save it\n",
    "tweet_time = hydro_tweets_times[tweet_nb].replace(minute=0, second=0)\n",
    "print(f\"Time of Trump's first tweet: {tweet_time}\")\n",
    "start_time = tweet_time - timedelta(hours=48)\n",
    "end_time = tweet_time + timedelta(hours=48)\n",
    "\n",
    "# # Hydro data \n",
    "# folder = 'data'\n",
    "# filename = 'hydroxychloroquine'\n",
    "# pagename = 'Hydroxychloroquine'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# # Control data\n",
    "# folder = 'data'\n",
    "# filename = 'climate'\n",
    "# pagename = 'Climate'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'coffee'\n",
    "# pagename = 'Coffee'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'news'\n",
    "# pagename = 'News'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'shop'\n",
    "# pagename = 'Shop'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'time'\n",
    "# pagename = 'Time'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hydro json\n",
    "folder = 'data'\n",
    "filename = 'hydroxychloroquine'\n",
    "gtrends_hydro_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "# Load control json\n",
    "folder = 'data'\n",
    "filename = 'climate'\n",
    "gtrends_climate_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'coffee'\n",
    "gtrends_coffee_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'news'\n",
    "gtrends_news_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "folder = 'data'\n",
    "filename = 'shop'\n",
    "gtrends_shop_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'time'\n",
    "gtrends_time_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "# Combine the dataframes\n",
    "gtrends_df = pd.concat([gtrends_hydro_df, gtrends_climate_df, gtrends_coffee_df, \n",
    "                        gtrends_news_df, gtrends_shop_df, gtrends_time_df],\n",
    "                         axis=1, keys=['hydro', 'climate', 'coffee', 'news',\n",
    "                                        'shop', 'time'])\n",
    "gtrends_df.columns = gtrends_df.columns.droplevel(1)\n",
    "\n",
    "gtrends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Causal Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the pre-period (time before Trump's tweet) and the post-period (the time after his tweet) for the causal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose pre and post-periods\n",
    "pre_period = [gtrends_df.index[0], tweet_time]\n",
    "post_period = [tweet_time+timedelta(hours=1), gtrends_df.index[-1]]\n",
    "print(pre_period)\n",
    "print(post_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify the assumption that the test variable can be modelled as a linear regression with the covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify assumptions on pre-period: linear regression\n",
    "mod = smf.ols(formula='hydro ~ coffee + climate + news + shop + time', data=gtrends_df[:pre_period[1]])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values associated with several coefficientss are significant (< 0.05), and the R² = 0.54. The fit is therefore pretty good, but the results of our causal analysis should still be interpreted with caution.\n",
    "\n",
    "We can now run the causal impact analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct causal\n",
    "impact = CausalImpact(data = gtrends_df, pre_period=pre_period, post_period=post_period, prior_level_sd=None, model_args={'dynamic_regression': True})\n",
    "impact.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The causal impact analysis corroborates the visual inspection of the Google Trends time series: the peak in interest on hydroxychloroquine preceded Trump's first tweet. His tweet therefore doesn't seem to have had a strong impact on interest. Note, however, that we are using global trends time series. It might be that Trump had a local impact (though probably not strong) on Google searches, e.g. in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Alternative Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, an alternative possibility to explain why online trends time series and Trump's tweets Granger cause each other is that an external factor is the \"real\" cause of the surge of interest.\n",
    "\n",
    "Some online research revealed that the big peak in interest mid-March might have caused by the two following major events, which both took place on March 16, 2020:\n",
    "- a mobility changepoint in the United States following restrictions, according to [Manoel et al.](https://arxiv.org/abs/2005.08505),\n",
    "- \"A study on the use of hydroxychloroquine in patients with SARS-CoV-2 was published (online via Youtube) - The preliminary data from this small study was heard round the world\", as quoted from [Saag et al.](https://jamanetwork.com/journals/jama/fullarticle/2772921).\n",
    "\n",
    "We therefore tried to reconduct a causal impact analysis, considering March 16, 5 P.M. (GMT, so between 9 A.M and 12 A.M. in the USA) as our intervention time.\n",
    "\n",
    "Again, we start by fetching the global data over a window of 4 days (not exactly centred around the chosen intervention time due to data unavailability on Google Trends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request data from GTrends API (both hydro and Google) and save it\n",
    "treatment_time = pd.Timestamp(2020,3,16,17,0,0)\n",
    "print(f\"Tweet time: {treatment_time}\")\n",
    "start_time = pd.Timestamp(2020,3,15,12,0,0)\n",
    "end_time = pd.Timestamp(2020,3,19,12,0,0)\n",
    "\n",
    "print(f\"start time: {start_time}\")\n",
    "print(f\"end time: {end_time}\")\n",
    "\n",
    "alias = \"publi\"\n",
    "\n",
    "# Hydro data \n",
    "# folder = 'data'\n",
    "# filename = 'hydroxychloroquine'\n",
    "# pagename = 'Hydroxychloroquine'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# # Control data\n",
    "# folder = 'data'\n",
    "# filename = 'climate'\n",
    "# pagename = 'Climate'y\n",
    "# folder = 'data'\n",
    "# filename = 'coffee'\n",
    "# pagename = 'Coffee'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'news'\n",
    "# pagename = 'News'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'shop'\n",
    "# pagename = 'Shop'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)\n",
    "\n",
    "# folder = 'data'\n",
    "# filename = 'time'\n",
    "# pagename = 'Time'\n",
    "# request_gtrends(folder, filename, alias, pagename, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hydro json\n",
    "folder = 'data'\n",
    "filename = 'hydroxychloroquine'\n",
    "gtrends_hydro_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "# Load control json\n",
    "folder = 'data'\n",
    "filename = 'climate'\n",
    "gtrends_climate_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'coffee'\n",
    "gtrends_coffee_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'news'\n",
    "gtrends_news_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'shop'\n",
    "gtrends_shop_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "folder = 'data'\n",
    "filename = 'time'\n",
    "gtrends_time_df = load_gtrend_hourly_df(folder, filename, alias)\n",
    "\n",
    "# Combine the dataframes\n",
    "gtrends_df = pd.concat([gtrends_hydro_df, gtrends_climate_df, gtrends_coffee_df, \n",
    "                        gtrends_news_df, gtrends_shop_df, gtrends_time_df],\n",
    "                         axis=1, keys=['hydro', 'climate', 'coffee', 'news',\n",
    "                                        'shop', 'time'])\n",
    "gtrends_df.columns = gtrends_df.columns.droplevel(1)\n",
    "\n",
    "gtrends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select the pre- and post-periods for the causal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose pre and post-periods\n",
    "pre_period = [gtrends_df.index[0], treatment_time]\n",
    "post_period = [treatment_time+timedelta(hours=1), gtrends_df.index[-1]]\n",
    "print(pre_period)\n",
    "print(post_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting the analysis, we verify the assumption that the time series can me modelled by a linear regression in the pre-period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify assumptions on pre-period: linear regression\n",
    "mod = smf.ols(formula='hydro ~ coffee + climate + news + shop + time', data=gtrends_df[:pre_period[1]])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several coefficients show statistical significance (p-value < 0.05) and the R²=0.52 is fairly high. Still, the results should once more be interpreted with caution.\n",
    "\n",
    "We can now run the causal impact analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct causal\n",
    "impact = CausalImpact(data = gtrends_df, pre_period=pre_period, post_period=post_period, prior_level_sd=None, model_args={'dynamic_regression': True})\n",
    "impact.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The causal impact analysis shows that the intervention on March 16 (the mobility restrictions, the publication or both - perhaps even other events, this period was quite hectic) has a strong impact on Google Trends searches on Wikipedia. \n",
    "\n",
    "In conclusion, Trump's tweets and online Trends are correlated, and both time series contain information that can predict the other (cf. Granger causality). A focus on the first tweet suggests that rather than the tweets causing public online interest or vice versa, it is quite likely that external events were the real cause of interest. This closer analysis was only conducted on one of his tweets and for one topic, however. The results should therefore be interpreted with caution, and a more systematic study should be done to generalize our observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
